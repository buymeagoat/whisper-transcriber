# T031: Production Deployment and Monitoring - Enhanced Docker Compose
# Production-ready configuration with advanced monitoring and optimization
version: '3.8'

services:
  # ============================================================================
  # Load Balancer and SSL Termination
  # ============================================================================
  nginx:
    image: nginx:1.25-alpine@sha256:2d194184b067db3598771b4cf326cfe6ad5051937ba1132b8b7d4b0184e0d0a6
    container_name: whisper-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "9090:9090"  # Prometheus (authenticated)
      - "3000:3000"  # Grafana (authenticated)
    volumes:
      - ./nginx/nginx.prod.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./ssl:/etc/ssl/certs:ro
      - nginx_logs:/var/log/nginx
      - nginx_cache:/var/cache/nginx
    depends_on:
      app:
        condition: service_healthy
    networks:
      - frontend
      - backend
      - monitoring
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETUID
      - SETGID
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /var/cache/nginx:noexec,nosuid,size=500m
      - /var/run:noexec,nosuid,size=10m
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.2'
          memory: 128M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # Database with Enhanced Performance
  # ============================================================================
  postgres:
    image: postgres:15-alpine@sha256:7cffd597c26cdc7981a8422e94138a82be1b501f88bf05e9af9c044bff83bedb
    container_name: whisper-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-whisper_prod}
      POSTGRES_USER: ${POSTGRES_USER:-whisper}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "--data-checksums --auth-host=scram-sha-256"
      # Performance optimization
      POSTGRES_SHARED_BUFFERS: ${POSTGRES_SHARED_BUFFERS:-256MB}
      POSTGRES_EFFECTIVE_CACHE_SIZE: ${POSTGRES_EFFECTIVE_CACHE_SIZE:-1GB}
      POSTGRES_WORK_MEM: ${POSTGRES_WORK_MEM:-16MB}
      POSTGRES_MAINTENANCE_WORK_MEM: ${POSTGRES_MAINTENANCE_WORK_MEM:-64MB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_backups:/backups
      - postgres_logs:/var/log/postgresql
      - ./postgres/init:/docker-entrypoint-initdb.d:ro
      - ./postgres/postgresql.prod.conf:/etc/postgresql/postgresql.conf:ro
    expose:
      - "5432"
    networks:
      - backend
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID
      - SETGID
      - DAC_OVERRIDE
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=200m
      - /var/run/postgresql:noexec,nosuid,size=50m
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-whisper} -d ${POSTGRES_DB:-whisper_prod}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # ============================================================================
  # Redis with Clustering Support
  # ============================================================================
  redis:
    image: redis:7-alpine@sha256:de13e74e14b98eb96bdf886791ae47686c3c5d29f9d5f85ea55206843e3fce26
    container_name: whisper-redis
    restart: unless-stopped
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD}
    expose:
      - "6379"
    volumes:
      - redis_data:/data
      - redis_logs:/var/log/redis
      - ./redis/redis.prod.conf:/usr/local/etc/redis/redis.conf:ro
    command: redis-server /usr/local/etc/redis/redis.conf --requirepass ${REDIS_PASSWORD}
    networks:
      - backend
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID
      - SETGID
    user: "999:999"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 256M
    healthcheck:
      test: ["CMD", "redis-cli", "--no-auth-warning", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # Main Application with Auto-scaling Support
  # ============================================================================
  app:
    build:
      context: .
      dockerfile: Dockerfile.optimized
      target: production
      args:
        INSTALL_DEV: false
        ENABLE_HEALTH_CHECKS: true
        OPTIMIZE_SIZE: true
    image: whisper-transcriber:production
    container_name: whisper-app
    restart: unless-stopped
    environment:
      # Database configuration
      DATABASE_URL: postgresql://${POSTGRES_USER:-whisper}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-whisper_prod}?sslmode=prefer
      DB_URL: postgresql://${POSTGRES_USER:-whisper}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-whisper_prod}?sslmode=prefer
      
      # Redis configuration
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/1
      
      # Security configuration
      SECRET_KEY: ${SECRET_KEY}
      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
      CORS_ORIGINS: ${CORS_ORIGINS:-https://yourdomain.com}
      
      # Application configuration
      APP_ENV: production
      DEBUG: false
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      MAX_UPLOAD_SIZE: ${MAX_UPLOAD_SIZE:-1GB}
      
      # Monitoring and observability
      SENTRY_DSN: ${SENTRY_DSN:-}
      PROMETHEUS_ENABLED: true
      METRICS_PORT: 9091
      OPENTELEMETRY_ENABLED: ${OPENTELEMETRY_ENABLED:-true}
      JAEGER_ENDPOINT: ${JAEGER_ENDPOINT:-http://jaeger:14268/api/traces}
      
      # Performance tuning
      WORKER_PROCESSES: ${WORKER_PROCESSES:-4}
      WORKER_CONNECTIONS: ${WORKER_CONNECTIONS:-1000}
      KEEPALIVE_TIMEOUT: 120
      CLIENT_TIMEOUT: 300
      
      # Resource limits
      MAX_CONCURRENT_JOBS: ${MAX_CONCURRENT_JOBS:-10}
      MEMORY_LIMIT_MB: ${MEMORY_LIMIT_MB:-3072}
      
    volumes:
      - app_storage:/app/storage
      - app_uploads:/app/storage/uploads
      - app_transcripts:/app/storage/transcripts
      - app_logs:/app/logs
      - app_cache:/app/cache
      - app_temp:/app/temp
      - models_cache:/app/models
    expose:
      - "8000"
      - "9091"  # Metrics endpoint
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - backend
      - monitoring
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=1g
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 1G
      # Auto-scaling configuration (for Swarm mode)
      replicas: ${APP_REPLICAS:-2}
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # ============================================================================
  # Celery Workers with Auto-scaling
  # ============================================================================
  worker:
    build:
      context: .
      dockerfile: Dockerfile.optimized
      target: production
      args:
        INSTALL_DEV: false
        ENABLE_HEALTH_CHECKS: false
        OPTIMIZE_SIZE: true
    image: whisper-transcriber:production
    restart: unless-stopped
    command: ["celery", "-A", "api.celery_app", "worker", "--loglevel=info", "--concurrency=${CELERY_CONCURRENCY:-2}", "--prefetch-multiplier=1"]
    environment:
      # Same core configuration as app
      DATABASE_URL: postgresql://${POSTGRES_USER:-whisper}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-whisper_prod}?sslmode=prefer
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/1
      SECRET_KEY: ${SECRET_KEY}
      APP_ENV: production
      DEBUG: false
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      
      # Worker-specific configuration
      CELERY_WORKER_MAX_MEMORY_PER_CHILD: ${CELERY_WORKER_MAX_MEMORY_PER_CHILD:-1000000}
      CELERY_WORKER_HIJACK_ROOT_LOGGER: false
      CELERY_WORKER_LOG_FORMAT: '[%(asctime)s: %(levelname)s/%(processName)s] %(message)s'
      
      # Performance tuning
      WHISPER_CACHE_DIR: /app/cache/whisper
      TORCH_HOME: /app/cache/torch
      
    volumes:
      - app_storage:/app/storage
      - app_uploads:/app/storage/uploads
      - app_transcripts:/app/storage/transcripts
      - app_logs:/app/logs
      - app_cache:/app/cache
      - app_temp:/app/temp
      - models_cache:/app/models
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - backend
      - monitoring
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=2g
    deploy:
      resources:
        limits:
          cpus: '3.0'
          memory: 3G
        reservations:
          cpus: '1.0'
          memory: 1G
      # Worker scaling
      replicas: ${WORKER_REPLICAS:-2}
      update_config:
        parallelism: 1
        delay: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # ============================================================================
  # Celery Beat Scheduler
  # ============================================================================
  scheduler:
    build:
      context: .
      dockerfile: Dockerfile.optimized
      target: production
    image: whisper-transcriber:production
    container_name: whisper-scheduler
    restart: unless-stopped
    command: ["celery", "-A", "api.celery_app", "beat", "--loglevel=info", "--scheduler=django_celery_beat.schedulers:DatabaseScheduler"]
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-whisper}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-whisper_prod}?sslmode=prefer
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      CELERY_BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD}@redis:6379/1
      SECRET_KEY: ${SECRET_KEY}
      APP_ENV: production
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    volumes:
      - app_logs:/app/logs
      - app_cache:/app/cache
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - backend
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================================
  # Enhanced Monitoring Stack
  # ============================================================================
  prometheus:
    image: prom/prometheus:v2.47.0@sha256:e5ffa79cc96734d3cace6c9f4de45a9f7c2e16ce0bb6ce5b3b1b07aba5b1cc6f
    container_name: whisper-prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=${PROMETHEUS_RETENTION:-30d}'
      - '--storage.tsdb.retention.size=${PROMETHEUS_RETENTION_SIZE:-10GB}'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--web.external-url=https://${DOMAIN:-localhost}/prometheus/'
      - '--web.route-prefix=/prometheus/'
    volumes:
      - ./monitoring/prometheus/prometheus.prod.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    expose:
      - "9090"
    depends_on:
      - app
    networks:
      - backend
      - monitoring
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Grafana with Enhanced Dashboards
  grafana:
    image: grafana/grafana:10.1.0@sha256:3e8e99e1c2c5fd65f47ee2c35b8937a81e1c5a98e4b6c41e4b0a24a9e3de7f1c
    container_name: whisper-grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_SECURITY_ALLOW_EMBEDDING: true
      GF_AUTH_ANONYMOUS_ENABLED: false
      GF_INSTALL_PLUGINS: grafana-piechart-panel,grafana-clock-panel,grafana-simple-json-datasource
      GF_SERVER_ROOT_URL: https://${DOMAIN:-localhost}/grafana/
      GF_SERVER_SERVE_FROM_SUB_PATH: true
      GF_SMTP_ENABLED: ${GRAFANA_SMTP_ENABLED:-false}
      GF_SMTP_HOST: ${GRAFANA_SMTP_HOST:-}
      GF_SMTP_USER: ${GRAFANA_SMTP_USER:-}
      GF_SMTP_PASSWORD: ${GRAFANA_SMTP_PASSWORD:-}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
      - ./monitoring/grafana/alerting:/etc/grafana/provisioning/alerting:ro
    expose:
      - "3000"
    depends_on:
      - prometheus
    networks:
      - backend
      - monitoring
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Node Exporter for System Metrics
  node_exporter:
    image: prom/node-exporter:v1.6.1@sha256:fc9b5bc803f2a1fb574bd35ac7d3bb1c89e7b4d6ba7beb5a1e52e3c3e3e3e3e3
    container_name: whisper-node-exporter
    restart: unless-stopped
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    expose:
      - "9100"
    networks:
      - monitoring
    security_opt:
      - no-new-privileges:true
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 64M

  # cAdvisor for Container Metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2@sha256:3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b3b
    container_name: whisper-cadvisor
    restart: unless-stopped
    privileged: true
    devices:
      - /dev/kmsg
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    expose:
      - "8080"
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          cpus: '0.3'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M

  # ============================================================================
  # Log Aggregation and Analysis
  # ============================================================================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.9.0@sha256:4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e4e
    container_name: whisper-elasticsearch
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    expose:
      - "9200"
    networks:
      - backend
      - monitoring
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  # Filebeat for Log Shipping
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.9.0@sha256:5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e
    container_name: whisper-filebeat
    restart: unless-stopped
    user: root
    volumes:
      - ./monitoring/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - app_logs:/var/log/app:ro
      - nginx_logs:/var/log/nginx:ro
      - postgres_logs:/var/log/postgresql:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - elasticsearch
    networks:
      - monitoring

# ============================================================================
# Volumes for Data Persistence
# ============================================================================
volumes:
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/postgres
  postgres_backups:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${BACKUP_DIR:-./backups}/postgres
  postgres_logs:
    driver: local
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/redis
  redis_logs:
    driver: local
  app_storage:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/storage
  app_uploads:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/uploads
  app_transcripts:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/transcripts
  app_logs:
    driver: local
  app_cache:
    driver: local
  app_temp:
    driver: local
  models_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/models
  prometheus_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/prometheus
  grafana_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/grafana
  elasticsearch_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_DIR:-./data}/elasticsearch
  nginx_logs:
    driver: local
  nginx_cache:
    driver: local

# ============================================================================
# Network Configuration
# ============================================================================
networks:
  frontend:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/24
          gateway: 172.20.0.1
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.bridge.enable_ip_masquerade: "true"
      com.docker.network.driver.mtu: 1500

  backend:
    driver: bridge
    internal: true
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/24
          gateway: 172.21.0.1
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.driver.mtu: 1500

  monitoring:
    driver: bridge
    internal: true
    ipam:
      driver: default
      config:
        - subnet: 172.22.0.0/24
          gateway: 172.22.0.1
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.driver.mtu: 1500