# Observability Guide

This document captures the core observability signals that ship with Whisper
Transcriber. It focuses on three pillars: health checks, structured logging,
and Prometheus metrics. Use these notes as the canonical reference when wiring
alerts or connecting the service to your telemetry backend.

## Health probes

Whisper Transcriber exposes two HTTP probes from `api/routes/health.py`:

- `GET /livez` – a cheap liveness probe used by Docker to verify the container
  is still running.
- `GET /readyz` – verifies database connectivity, Redis availability, and that
  the configured Whisper models are discoverable on disk. The endpoint responds
  with HTTP `503` when any dependency is degraded.

The Docker image and the `docker-compose.yml` file both rely on `/readyz` for
runtime health checks. Keep the response time for this endpoint under
approximately one second to avoid cascading restarts.

## Structured logging

`api/utils/logger.py` standardises all log output as single-line JSON records.
The formatter emits the following schema for every record. All fields are
strings unless noted otherwise.

| Field | Type | Description |
| --- | --- | --- |
| `timestamp` | string (ISO-8601) | UTC timestamp for when the record was emitted. |
| `level` | string | Logging level (e.g. `INFO`, `ERROR`). |
| `logger` | string | Logger name (usually module-qualified). |
| `message` | string | Human-readable summary. |
| `request_id` | string, nullable | Correlates logs for a single HTTP request. Generated by the access middleware. |
| `job_id` | string, nullable | Present for job queue activity and Celery worker logs. |
| `latency_ms` | number, nullable | Request or task latency in milliseconds when provided. |
| `module` | string | Python module name. |
| `function` | string | Function that emitted the record. |
| `line` | number | Source line number. |
| `process_id` | number | OS process identifier. |
| `thread_id` | number | Python thread identifier. |
| `exception` | string, optional | Present when the record includes exception info. |
| `stack` | string, optional | Present when stack info is logged. |
| `extra` | object, optional | Additional context passed via `logger.*(..., extra={...})`. |

### Logging conventions

- Use `bind_request_id`, `bind_job_id`, and `bind_latency` to enrich log records
  with correlation identifiers in long-running workflows.
- Always prefer `logger.info("HTTP request completed", extra={...})` to
  pre-serialised JSON strings; the formatter takes care of encoding.
- When emitting exceptions, rely on `logger.exception` or `exc_info=True` so the
  structured record includes a full stack trace.

### Alert thresholds for logs

Log-based alerts complement metric alerts and help surface regressions quickly.
Consider the following baselines:

- **Authentication failures**: trigger an alert when more than five `ERROR`
  level access log entries with HTTP 401/403 occur within a five-minute window.
- **Job failures**: alert on any sustained stream (>3 events in 10 minutes) of
  `ERROR` or `CRITICAL` log lines that include a `job_id` and the message
  substring `failed`.

## Prometheus metrics

The `/metrics` endpoint publishes RED/USE metrics together with job queue
insights. Key metric names and their meanings are listed below.

| Metric | Type | Labels | Description |
| --- | --- | --- | --- |
| `whisper_http_requests_total` | Counter | `method`, `endpoint`, `status_code` | Total HTTP requests served. |
| `whisper_http_request_duration_seconds` | Histogram | `method`, `endpoint` | Latency distribution for HTTP requests. |
| `whisper_http_requests_in_progress` | Gauge | `endpoint` | Number of inflight HTTP requests. |
| `whisper_jobs_total` | Gauge | `status` | Count of jobs in each status (queued, processing, completed, failed, …). |
| `whisper_job_queue_depth` | Gauge | – | Total jobs waiting for workers (`status="queued"`). |
| `whisper_job_duration_seconds` | Histogram | – | Processing durations for recently completed jobs. |
| `whisper_worker_failures` | Gauge | `status` | Count of jobs in each failure bucket. |
| `whisper_resource_utilization_ratio` | Gauge | `resource` | CPU, memory, disk, and Redis memory utilisation ratios. |
| `whisper_resource_saturation_ratio` | Gauge | `resource` | Saturation ratios for CPU, Redis connection pool, etc. |
| `whisper_resource_errors_total` | Counter | `resource`, `kind` | Number of collection failures for resource metrics. |

### Alert thresholds for metrics

Use the following starting points when wiring alert rules:

- **Job queue depth** (`whisper_job_queue_depth`): warn at >25 for five minutes;
  critical at >50 for ten minutes. Sustained growth usually indicates workers
  are undersized or offline.
- **Worker failures** (`whisper_worker_failures`): alert when any failure label
  stays above zero for more than two consecutive scrapes.
- **HTTP latency** (`whisper_http_request_duration_seconds`): investigate when
  the P95 bucket exceeds two seconds for more than fifteen minutes.
- **CPU utilisation** (`whisper_resource_utilization_ratio{resource="cpu"}`):
  warn at >0.8 for ten minutes to catch runaway workloads early.

Tune the exact thresholds per environment once you have baseline data from
production traffic or load tests.
